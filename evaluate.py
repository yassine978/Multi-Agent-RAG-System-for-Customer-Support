"""Evaluation script for the TelecomPlus support agent.

This script evaluates the agent's responses against the expected answers
from the evaluation dataset.

Students should modify this script to:
1. Call their actual agent implementation
2. Implement proper evaluation logic (e.g., LLM-as-a-judge)
3. Calculate meaningful metrics (accuracy, relevance, etc.)
"""

import os
import json
import pandas as pd
from google import genai
from google.genai import types
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

from src.main import answer

# Configure Google Gemini API client
_client = None

def get_gemini_client():
    """Get or create Gemini client (lazy initialization)."""
    global _client
    if _client is None:
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            raise ValueError("GOOGLE_API_KEY environment variable is required for evaluation")
        _client = genai.Client(api_key=api_key)
    return _client

# Batch size for evaluation (evaluate multiple responses in one API call)
BATCH_SIZE = 5


def evaluate_batch(batch: list[dict]) -> list[dict]:
    """Evaluate a batch of responses using LLM-as-a-judge.

    This approach optimizes API usage by evaluating multiple Q&A pairs in a single call.
    Focus is on semantic meaning rather than exact word matching.

    Args:
        batch: List of dicts with 'question', 'expected', 'agent_answer', 'index'

    Returns:
        List of dicts with 'index', 'score' (0-1), 'explanation'
    """
    # Build the evaluation prompt with all batch items
    items_text = ""
    for i, item in enumerate(batch):
        items_text += f"""
--- ITEM {i+1} ---
Question: {item['question']}
Expected Answer: {item['expected']}
Agent Answer: {item['agent_answer']}
"""

    evaluation_prompt = f"""You are an evaluation judge for a customer support AI system.
Your task is to evaluate if the agent's answers convey the same essential information as the expected answers.

IMPORTANT EVALUATION CRITERIA:
- Focus on MEANING and KEY INFORMATION, not exact wording
- Score 1 if the agent's answer contains the essential information from the expected answer
- Score 0.5 if the agent's answer is partially correct (some key info missing or minor errors)
- Score 0 if the agent's answer is wrong, misleading, or missing critical information
- Be lenient with formatting differences, synonyms, and rephrasing
- Numbers and specific data MUST be accurate

Evaluate these {len(batch)} items:
{items_text}

Respond in JSON format with an array of evaluations:
{{
    "evaluations": [
        {{"item": 1, "score": <0, 0.5, or 1>, "reason": "<brief explanation>"}},
        ...
    ]
}}

Only output valid JSON, nothing else."""

    try:
        client = get_gemini_client()
        response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=evaluation_prompt,
            config=types.GenerateContentConfig(
                temperature=0,  # Deterministic evaluation
                response_mime_type="application/json"
            )
        )

        result = json.loads(response.text)
        evaluations = result.get("evaluations", [])

        # Map back to original indices
        output = []
        for i, item in enumerate(batch):
            eval_result = evaluations[i] if i < len(evaluations) else {"score": 0, "reason": "Evaluation failed"}
            output.append({
                "index": item["index"],
                "score": eval_result.get("score", 0),
                "explanation": eval_result.get("reason", "No explanation")
            })
        return output

    except Exception as e:
        print(f"Evaluation API error: {e}")
        # Return neutral scores on error
        return [{"index": item["index"], "score": 0.5, "explanation": f"Error: {e}"} for item in batch]


def evaluate_response(question: str, expected_answer: str, agent_answer: str) -> tuple[float, str]:
    """Evaluate a single response using LLM-as-a-judge.

    TODO: Students should implement actual evaluation logic here.
    Consider using:
    - LLM-as-a-judge (e.g., with OpenAI/Anthropic API)
    - Semantic similarity (e.g., with sentence transformers)
    - Custom scoring based on key information extraction

    Args:
        question: The customer question
        expected_answer: The expected answer from the dataset
        agent_answer: The answer generated by your agent

    Returns:
        Tuple of (score, explanation)
    """
    batch = [{"question": question, "expected": expected_answer, "agent_answer": agent_answer, "index": 0}]
    results = evaluate_batch(batch)
    return results[0]["score"], results[0]["explanation"]


def run_evaluation():
    """Run evaluation on all questions from the evaluation dataset.

    Optimized for API efficiency:
    1. First, collect all agent answers
    2. Then batch evaluate using LLM-as-a-judge (5 items per API call)
    This reduces API calls from 25 to just 5 for evaluation.
    """

    # Load evaluation questions
    print("Loading evaluation questions...")
    df = pd.read_excel("data/evaluation_questions.xlsx")

    print(f"Loaded {len(df)} questions\n")
    print("=" * 80)

    # Phase 1: Collect all agent answers
    print("\nüìù PHASE 1: Generating agent answers...")
    print("=" * 80)

    results = []
    for idx, row in df.iterrows():
        question = row["Question"]
        expected_answer = row["R√©ponse Attendue"]
        difficulty = row["Difficult√©"]

        print(f"\nQuestion {idx + 1}/{len(df)} [{difficulty}]:")
        print(f"Q: {question[:100]}...")

        # Get agent's answer
        # TODO: Students should call their actual agent here
        agent_answer = answer(question)
        print(f"Agent: {agent_answer[:150]}...")

        # Store result (score will be added later)
        results.append({
            "index": idx,
            "Question": question,
            "Expected Answer": expected_answer,
            "Agent Answer": agent_answer,
            "Difficulty": difficulty,
            "Score": 0,
            "Explanation": ""
        })

        print("-" * 80)

    # Phase 2: Batch evaluate all responses
    print("\n\n‚öñÔ∏è PHASE 2: Evaluating responses (batched LLM-as-a-judge)...")
    print("=" * 80)

    total_score = 0
    num_batches = (len(results) + BATCH_SIZE - 1) // BATCH_SIZE

    for batch_idx in range(num_batches):
        start_idx = batch_idx * BATCH_SIZE
        end_idx = min(start_idx + BATCH_SIZE, len(results))

        # Prepare batch for evaluation
        batch = []
        for i in range(start_idx, end_idx):
            batch.append({
                "question": results[i]["Question"],
                "expected": results[i]["Expected Answer"],
                "agent_answer": results[i]["Agent Answer"],
                "index": i
            })

        print(f"\nEvaluating batch {batch_idx + 1}/{num_batches} (questions {start_idx + 1}-{end_idx})...")

        # Evaluate batch
        eval_results = evaluate_batch(batch)

        # Update results with scores
        for eval_result in eval_results:
            idx = eval_result["index"]
            results[idx]["Score"] = eval_result["score"]
            results[idx]["Explanation"] = eval_result["explanation"]
            total_score += eval_result["score"]
            print(f"  Q{idx + 1}: Score={eval_result['score']} - {eval_result['explanation'][:60]}...")

    # Calculate final metrics
    accuracy = total_score / len(df)

    # Calculate metrics by difficulty
    difficulty_scores = {}
    for r in results:
        diff = r["Difficulty"]
        if diff not in difficulty_scores:
            difficulty_scores[diff] = {"total": 0, "count": 0}
        difficulty_scores[diff]["total"] += r["Score"]
        difficulty_scores[diff]["count"] += 1

    print("\n" + "=" * 80)
    print("üìä EVALUATION RESULTS")
    print("=" * 80)
    print(f"Total questions: {len(df)}")
    print(f"Total score: {total_score:.1f}/{len(df)}")
    print(f"Overall accuracy: {accuracy:.1%}")

    print("\nüìà Scores by difficulty:")
    for diff, scores in sorted(difficulty_scores.items()):
        diff_accuracy = scores["total"] / scores["count"]
        print(f"  {diff}: {scores['total']:.1f}/{scores['count']} ({diff_accuracy:.1%})")

    # Save results to Excel
    results_df = pd.DataFrame(results)
    results_df = results_df.drop(columns=["index"])  # Remove internal index
    results_df.to_excel("evaluation_results.xlsx", index=False)
    print("\n‚úÖ Results saved to: evaluation_results.xlsx")

    # Print API efficiency stats
    print(f"\nüí° API Efficiency: Used {num_batches} evaluation API calls instead of {len(df)}")


if __name__ == "__main__":
    run_evaluation()
